{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "30610b65-0ca2-42c9-a1e8-2acb7ed4d3e7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<dask.config.set at 0x7f276c446820>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import ERFutils\n",
    "import dask\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.optimize import curve_fit\n",
    "from scipy.signal import savgol_filter\n",
    "\n",
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import cftime\n",
    "import dask\n",
    "import xarrayutils\n",
    "import cartopy.crs as ccrs\n",
    "from xmip.preprocessing import combined_preprocessing\n",
    "from xmip.preprocessing import replace_x_y_nominal_lat_lon\n",
    "from xmip.drift_removal import replace_time\n",
    "from xmip.postprocessing import concat_experiments\n",
    "import xmip.drift_removal as xm_dr\n",
    "import xmip as xm\n",
    "import xesmf as xe\n",
    "import datetime\n",
    "from datetime import timedelta\n",
    "from dateutil.relativedelta import relativedelta\n",
    "import cf_xarray as cfxr\n",
    "import scipy.signal as signal\n",
    "from scipy.sparse import diags\n",
    "from scipy.sparse.linalg import spsolve_triangular\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib as mpl\n",
    "import cmocean\n",
    "import cmocean.cm as cmo\n",
    "from matplotlib.gridspec import GridSpec\n",
    "\n",
    "import copy\n",
    "import os\n",
    "\n",
    "from numpy import linalg as LA\n",
    "\n",
    "dask.config.set(**{'array.slicing.split_large_chunks': True})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56d26f69-c55c-4690-ac48-5a27b52ea380",
   "metadata": {},
   "source": [
    "# Load data and diagnose Green's Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cf6c2e0-52d6-4d5d-aaa1-9761815e9250",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_set = ERFutils.model_set\n",
    "A = ERFutils.A\n",
    "ds_out = ERFutils.ds_out\n",
    "\n",
    "plot = True\n",
    "savgol = True\n",
    "save = False\n",
    "\n",
    "train_id = ['1pctCO2']\n",
    "output_path = ERFutils.path_to_ERF_outputs\n",
    "\n",
    "for train in train_id:\n",
    "    # Load ERF data\n",
    "    ERF = {}\n",
    "    ERF_path = f'{output_path}ERF/ERF_{train}_smooth_all_ds.nc4'\n",
    "    ERF_ds = xr.open_dataset(ERF_path)\n",
    "    ERF[train] = ERFutils.ds_to_dict(ERF_ds)\n",
    "    \n",
    "    # Diagnose Green's Functions\n",
    "    ds_control, ds_exp, G_ds = ERFutils.create_multimodel_GF_set(ERF, train, model_set, savgol)\n",
    "    \n",
    "    # Plot Global Mean Green's Functions (Optional)\n",
    "    if plot:\n",
    "        ERFutils.plot_mean_Greens(G_ds, train, overlay = True, save_fig = False)\n",
    "        \n",
    "    G_ds2 = G_ds.mean(dim = 'model')\n",
    "    \n",
    "    # Save Green's Functions\n",
    "    if save:\n",
    "        G_ds.to_netcdf(f'{output_path}G_{train}_ERF_all_ds.nc4')\n",
    "        G_ds2.to_netcdf(f'{output_path}G_{train}_ERF_mean_ds.nc4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ca4f24a-5c04-4298-9f61-b26cd54b3f26",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_set = ERFutils.model_set\n",
    "A = ERFutils.A\n",
    "ds_out = ERFutils.ds_out\n",
    "\n",
    "train_id = ['1pctCO2']\n",
    "output_path = ERFutils.path_to_ERF_outputs\n",
    "\n",
    "for train in train_id:\n",
    "    # Load ERF data\n",
    "    ERF = {}\n",
    "    ERF_path = f'{output_path}ERF/ERF_{train}_all_ds.nc4'\n",
    "    ERF_ds = xr.open_dataset(ERF_path)\n",
    "    ERF[train] = ERFutils.ds_to_dict(ERF_ds)\n",
    "    \n",
    "    tas_path = f'{output_path}tas/tas_CMIP_{train}_all_ds.nc4'\n",
    "    tas_ds = xr.open_dataset(tas_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8a8d226-ece6-41cb-a97b-c79ce0669e15",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10,5))\n",
    "for m in model_set:\n",
    "    ax.plot(ERF[train][m].ERF.values,alpha=0.5,c='gray')\n",
    "ax.plot(ERFutils.concat_multirun(ERF[train],'model').mean(dim = 'model').ERF.values,c = 'k', linewidth=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "088a6049-19f4-488d-a19a-0f6f289036f9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# function to perform locally weighted linear regression\n",
    "def local_weighted_regression(x0, X, Y, tau):\n",
    "    # add bias term\n",
    "    x0 = np.r_[1, x0]\n",
    "    X = np.c_[np.ones(len(X)), X]\n",
    "     \n",
    "    # fit model: normal equations with kernel\n",
    "    xw = X.T * weights_calculate(x0, X, tau)\n",
    "    theta = np.linalg.pinv(xw @ X) @ xw @ Y\n",
    "    # \"@\" is used to\n",
    "    # predict value\n",
    "    return x0 @ theta\n",
    "\n",
    "def weights_calculate(x0, X, tau):\n",
    "    return np.exp(np.sum((X - x0) ** 2, axis=1) / (-2 * (tau **2) ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4190a2a8-b1f2-49b6-b3db-f75b89096ab1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_id = ['1pctCO2']\n",
    "output_path = ERFutils.path_to_ERF_outputs\n",
    "A = ERFutils.A\n",
    "\n",
    "for train in train_id:\n",
    "    # Load ERF data\n",
    "    ERF = {}\n",
    "    ERF_path = f'{output_path}ERF/ERF_{train}_all_ds.nc4'\n",
    "    ERF_ds = xr.open_dataset(ERF_path)\n",
    "    \n",
    "    #ERF_path_hist = f'{output_path}ERF/ERF_historical_all_ds.nc4'\n",
    "    #ERF_ssp = xr.open_dataset(ERF_path)\n",
    "    #ERF_hist = xr.open_dataset(ERF_path_hist)\n",
    "    #ERF_ds = xr.concat([ERF_hist,ERF_ssp.assign_coords(year = range(165,250))],dim = 'year')\n",
    "    \n",
    "    ERF[train] = ERFutils.ds_to_dict(ERF_ds)\n",
    "    \n",
    "    tas_path = f'{output_path}tas/tas_CMIP_{train}_all_ds.nc4'\n",
    "    tas_ds = xr.open_dataset(tas_path)\n",
    "    \n",
    "ERF_all = ERFutils.concat_multirun(ERF[train],'model').mean(dim = 'model')\n",
    "tas_glob_mean = tas_ds.weighted(A).mean(dim = ['lat','lon']).mean(dim = ['model'])\n",
    "tas_glob_mean = tas_glob_mean.rename({'s': 'year'})\n",
    "\n",
    "#if 'ssp' in train:\n",
    "#    tas_glob_mean = tas_glob_mean.sel(year = slice(165,250)).assign_coords(year = range(0,85))\n",
    "\n",
    "tas_glob_mean = tas_glob_mean.sel(year = slice(ERF_all['year'].min(), ERF_all['year'].max()))\n",
    "X1 = ERF_all.year.values\n",
    "X2 = tas_glob_mean.year.values\n",
    "\n",
    "\n",
    "tau = 20\n",
    "j = 4\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10,5))\n",
    "Y1 = ERF_all.ERF.values# - ERF_all.ERF.values[j-1]\n",
    "Y2 = tas_glob_mean.tas.values# - tas_glob_mean.tas.values[j-1]\n",
    "N_years = len(ERF_all['year']) - j\n",
    "offsets = [i for i in range(0,-N_years,-1)]\n",
    "domain = np.linspace(ERF_all.year.values[j], ERF_all.year.values[-1], num=N_years)\n",
    "\n",
    "ERF_pred = [local_weighted_regression(x0, X1, Y1, tau) for x0 in domain]\n",
    "tas_pred = [local_weighted_regression(x0, X2, Y2, tau) for x0 in domain]\n",
    "input_matrix = diags(ERF_pred,offsets=offsets,shape=(N_years,N_years),format='csr')\n",
    "array_mat = input_matrix.toarray()\n",
    "cond_num = LA.cond(array_mat)\n",
    "#if cond_num > 10000:\n",
    "#    continue\n",
    "\n",
    "G = spsolve_triangular(input_matrix,tas_pred,lower=True)\n",
    "ax.plot(G)\n",
    "#if any(G[0:20] < 0) or G[0] < G[1]:\n",
    "#    continue\n",
    "\n",
    "#conv = signal.convolve(G,ERF_all.ERF.values,'full')\n",
    "    \n",
    "ax.set_title(f'tau = {tau}')\n",
    "ax.legend()\n",
    "\n",
    "# 1pctCO2 no zero (manual) - tau = 20, j = 4 (0.464)\n",
    "# 1pctCO2 no zero - tau = 21, j = 2 (0.4105)\n",
    "# 1pctCO2 zero - tau = 11, j = 4 (0.817)\n",
    "\n",
    "# ssp126 zero, hist - tau = 15, j = 160\n",
    "# ssp126 zero, no hist - tau = 5, j = 1\n",
    "\n",
    "# ssp245 zero, hist - tau = 7, j = 160\n",
    "# ssp245 zero, no hist - tau = 24, j = 4 \n",
    "\n",
    "# ssp370 zero, hist - tau = 16, j = 160 (0.0368)\n",
    "## ssp370 zero, no hist - tau = 8, j = 9 (1.65)\n",
    "## ssp370 no zero, hist - tau = 5, j = 148 (0.854)\n",
    "# ssp370 no zero, no hist - tau = 5, j = 0 (0.068)\n",
    "\n",
    "# ssp585 zero, hist - tau = 14, j = 160\n",
    "# ssp585 zero, no hist - tau = 20, j = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48d9dbd9-7224-486a-a31a-b48d521f6321",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#from scipy.sparse import coo_array, block_diag\n",
    "\n",
    "# Have to create the Green's functions locally, stack data array\n",
    "stacked_response = tas_ds.stack(allpoints=['lat','lon']).mean(dim = ['model'])\n",
    "stacked_tas = stacked_response.tas.values\n",
    "\n",
    "#N_latlong = len(stacked_response.tas.values[0])\n",
    "\n",
    "#A = input_matrix\n",
    "#A_block = block_diag()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93e3de3d-a516-4052-98af-9fdc5553bf08",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Rows = years\n",
    "# Columns = latlon pairs\n",
    "from scipy.signal import savgol_filter\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "j = 15\n",
    "smooth_window = 150 - j\n",
    "poly_order = 1\n",
    "\n",
    "ERF_smooth = savgol_filter(ERF_all.ERF.values[j:], smooth_window, poly_order, axis=0)\n",
    "N_years = len(ERF_all['year']) - j\n",
    "offsets = [i for i in range(0,-N_years,-1)]\n",
    "input_matrix = diags(ERF_smooth,offsets=offsets,shape=(N_years,N_years),format='csr')\n",
    "\n",
    "# Want to apply smoothing across columns\n",
    "stacked_response = tas_ds.stack(allpoints=['lat','lon']).mean(dim = ['model']).sel(s=slice(j,N_years+j-1))\n",
    "stacked_tas = stacked_response.tas.values\n",
    "\n",
    "ax.plot(stacked_tas[:,0],label='raw')\n",
    "smoothed_tas = savgol_filter(stacked_tas, smooth_window, poly_order, axis=0)\n",
    "smooth_tas_glob = savgol_filter(tas_glob_mean.tas.values[j:], smooth_window, poly_order, axis=0)\n",
    "ax.plot(smoothed_tas[:,0],label='smooth')\n",
    "ax.plot(ERF_smooth,label='ERF')\n",
    "\n",
    "ax.legend()\n",
    "G_stacked = spsolve_triangular(input_matrix,smoothed_tas,lower=True)\n",
    "G_glob = spsolve_triangular(input_matrix,smooth_tas_glob,lower=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1579a93-9a2c-44a6-9cf0-148867ff5c65",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.plot(G.weighted(A).mean(dim = ['lat','lon'])['G[tas]'])\n",
    "#plt.plot(ERF_smooth)\n",
    "#plt.plot(smooth_tas_glob)\n",
    "plt.plot(G_glob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "192ee4c6-8293-40fa-ba02-cd41406f6e45",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.plot(G_stacked[:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d1e66143-30df-4eb3-80ed-18ef32332020",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def G_to_ds(G_stacked,N_years):\n",
    "    G = xr.Dataset(coords={'lon': ('lon', tas_ds.lon.values),\n",
    "                            'lat': ('lat', tas_ds.lat.values),\n",
    "                            's': ('s', range(N_years))})\n",
    "    G = G.stack(allpoints=['lat','lon'])\n",
    "    G['G[tas]'] = (('year','allpoints'),G_stacked)\n",
    "    G = G.unstack('allpoints')\n",
    "\n",
    "    G['s'] = G['s'] - G['s'][0]\n",
    "    return G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0baccb91-9ae6-4eb4-b7d5-6304135ed7be",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_G_data(train, add_hist = False):\n",
    "    A = ERFutils.A\n",
    "    output_path = ERFutils.path_to_ERF_outputs\n",
    "    \n",
    "    # Load tas data\n",
    "    tas_path = f'{output_path}tas/tas_CMIP_{train}_all_ds.nc4'\n",
    "    tas_ds = (xr.open_dataset(tas_path)).mean(dim = 'model')\n",
    "    tas_glob_ds = (tas_ds.weighted(A).mean(dim = ['lat','lon'])).rename({'s': 'year'})\n",
    "    \n",
    "    # Load ERF data\n",
    "    ERF_path = f'{output_path}ERF/ERF_{train}_all_ds.nc4'\n",
    "    if add_hist:\n",
    "        ERF_path_hist = f'{output_path}ERF/ERF_historical_all_ds.nc4'\n",
    "        ERF_ssp = xr.open_dataset(ERF_path)\n",
    "        ERF_hist = xr.open_dataset(ERF_path_hist)\n",
    "        ERF_ds = xr.concat([ERF_hist,ERF_ssp.assign_coords(year = range(165,250))],dim = 'year')\n",
    "        \n",
    "    else:\n",
    "        ERF_ds = xr.open_dataset(ERF_path)\n",
    "\n",
    "    ERF_ds = ERFutils.concat_multirun(ERFutils.ds_to_dict(ERF_ds),'model').mean(dim = 'model')\n",
    "    \n",
    "    return tas_ds, tas_glob_ds, ERF_ds\n",
    "\n",
    "def l2_G(x):\n",
    "    \n",
    "    j, smooth_window, poly_order = [int(i) for i in x]\n",
    "    \n",
    "    N_years = len(ERF_ds['year']) - j\n",
    "    offsets = [i for i in range(0,-N_years,-1)]\n",
    "\n",
    "    smooth_ERF = savgol_filter(ERF_ds.ERF.values[j:], smooth_window, poly_order, axis=0)\n",
    "    smooth_tas = savgol_filter(tas_stacked[j:,:], smooth_window, poly_order, axis=0)\n",
    "    input_matrix = diags(smooth_ERF,offsets=offsets,shape=(N_years,N_years),format='csr')\n",
    "\n",
    "    G_stacked = spsolve_triangular(input_matrix,smooth_tas,lower=True)\n",
    "    G = G_to_ds(G_stacked,N_years)\n",
    "    \n",
    "    conv = signal.convolve(np.array(G.dropna(dim = 's')['G[tas]']), \n",
    "                               np.array(ERF_ds['ERF'])[~np.isnan(np.array(ERF_ds['ERF']))][..., None, None],\n",
    "                               'full')\n",
    "    conv = ERFutils.np_to_xr(conv, G, ERF_ds)\n",
    "    \n",
    "    return LA.norm((tas_ds.tas - conv.sel(s=slice(0,len(tas_ds.tas.values)-1))).weighted(A).mean(dim = ['lat','lon']))\n",
    "\n",
    "    #return np.mean(vector_norm(tas_ds.tas.values - conv.sel(s=slice(0,len(tas_ds.tas.values)-1)).values,dim=['lat','lon']).weighted(A).mean(dim = ['lat','lon']))\n",
    "\n",
    "def vector_norm(x, dim, ord=None):\n",
    "    return xr.apply_ufunc(np.linalg.norm, x,\n",
    "                          input_core_dims=[[dim]],\n",
    "                          kwargs={'ord': ord, 'axis': -1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6d7c2b12-e07b-4461-aae1-ee19d3dcb921",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New global optimum found, L2 = 2.217057674058305e+42, j = 0, window = 10, poly order = 1\n",
      "New global optimum found, L2 = 252532.3654, j = 0, window = 10, poly order = 2\n",
      "New global optimum found, L2 = 300.8575, j = 0, window = 30, poly order = 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cwomack/.conda/envs/gchp/lib/python3.9/site-packages/scipy/sparse/linalg/_dsolve/linsolve.py:743: RuntimeWarning: overflow encountered in true_divide\n",
      "  x[i] /= A.data[A_diagonal_index_row_i]\n",
      "/home/cwomack/.conda/envs/gchp/lib/python3.9/site-packages/scipy/signal/_signaltools.py:511: RuntimeWarning: invalid value encountered in multiply\n",
      "  ret = ifft(sp1 * sp2, fshape, axes=axes)\n",
      "/tmp/ipykernel_748/651522274.py:39: RuntimeWarning: Use of fft convolution on input with NAN or inf results in NAN or inf output. Consider using method='direct' instead.\n",
      "  conv = signal.convolve(np.array(G.dropna(dim = 's')['G[tas]']),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New global optimum found, L2 = 11.5022, j = 0, window = 110, poly order = 2\n",
      "New global optimum found, L2 = 4.6977, j = 0, window = 120, poly order = 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cwomack/.conda/envs/gchp/lib/python3.9/site-packages/scipy/sparse/linalg/_dsolve/linsolve.py:743: RuntimeWarning: overflow encountered in true_divide\n",
      "  x[i] /= A.data[A_diagonal_index_row_i]\n",
      "/tmp/ipykernel_748/651522274.py:39: RuntimeWarning: Use of fft convolution on input with NAN or inf results in NAN or inf output. Consider using method='direct' instead.\n",
      "  conv = signal.convolve(np.array(G.dropna(dim = 's')['G[tas]']),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New global optimum found, L2 = 3.1936, j = 1, window = 30, poly order = 2\n",
      "New global optimum found, L2 = 3.1757, j = 1, window = 40, poly order = 3\n",
      "New global optimum found, L2 = 2.8293, j = 1, window = 50, poly order = 1\n",
      "New global optimum found, L2 = 1.0539, j = 1, window = 50, poly order = 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cwomack/.conda/envs/gchp/lib/python3.9/site-packages/scipy/sparse/linalg/_dsolve/linsolve.py:743: RuntimeWarning: overflow encountered in true_divide\n",
      "  x[i] /= A.data[A_diagonal_index_row_i]\n",
      "/tmp/ipykernel_748/651522274.py:39: RuntimeWarning: Use of fft convolution on input with NAN or inf results in NAN or inf output. Consider using method='direct' instead.\n",
      "  conv = signal.convolve(np.array(G.dropna(dim = 's')['G[tas]']),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New global optimum found, L2 = 0.8928, j = 2, window = 50, poly order = 1\n",
      "New global optimum found, L2 = 0.7415, j = 2, window = 50, poly order = 3\n",
      "New global optimum found, L2 = 0.7054, j = 2, window = 70, poly order = 4\n",
      "New global optimum found, L2 = 0.5841, j = 2, window = 80, poly order = 2\n",
      "New global optimum found, L2 = 0.5233, j = 2, window = 120, poly order = 2\n",
      "New global optimum found, L2 = 0.4873, j = 3, window = 40, poly order = 2\n",
      "New global optimum found, L2 = 0.4768, j = 3, window = 60, poly order = 2\n",
      "New global optimum found, L2 = 0.4303, j = 3, window = 70, poly order = 2\n",
      "New global optimum found, L2 = 0.4148, j = 3, window = 80, poly order = 2\n",
      "New global optimum found, L2 = 0.4129, j = 3, window = 110, poly order = 2\n",
      "New global optimum found, L2 = 0.3977, j = 4, window = 20, poly order = 1\n",
      "New global optimum found, L2 = 0.3949, j = 4, window = 40, poly order = 1\n",
      "New global optimum found, L2 = 0.3864, j = 4, window = 60, poly order = 3\n",
      "New global optimum found, L2 = 0.3817, j = 4, window = 90, poly order = 4\n"
     ]
    }
   ],
   "source": [
    "from scipy.optimize import minimize\n",
    "\n",
    "train = '1pctCO2'\n",
    "\n",
    "# Load required data\n",
    "A = ERFutils.A\n",
    "tas_ds, tas_glob_ds, ERF_ds = load_G_data(train)\n",
    "tas_ds = tas_ds.sel(s = slice(ERF_ds['year'].min(), ERF_ds['year'].max()))\n",
    "tas_glob_ds = tas_glob_ds.sel(year = slice(ERF_ds['year'].min(), ERF_ds['year'].max()))\n",
    "\n",
    "# Used for local GFs, not necessary to recompute\n",
    "response_stacked = tas_ds.stack(allpoints=['lat','lon'])\n",
    "tas_stacked = response_stacked.tas.values\n",
    "\n",
    "x0 = [10,50,2]\n",
    "j_range = np.arange(0,15)\n",
    "smooth_range = np.arange(10,130,10)\n",
    "poly_range = [1,2,3,4]\n",
    "\n",
    "j_opt, smooth_opt, poly_opt, l2_opt = 0, 0, 0, 0\n",
    "\n",
    "l2_opt = np.Inf\n",
    "for j in j_range:\n",
    "    for smooth in smooth_range:\n",
    "        for p in poly_range:\n",
    "            l2_temp = l2_G([j,smooth,p])\n",
    "            if l2_temp < l2_opt:\n",
    "                j_opt, smooth_opt, poly_opt, l2_opt = j, smooth, p, l2_temp\n",
    "                print(f'New global optimum found, L2 = {round(float(l2_opt),4)}, j = {j}, window = {smooth}, poly order = {p}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f398d23a-3c79-4f37-ac97-7dc9225c1234",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "train = '1pctCO2'\n",
    "tau_range = np.arange(1,25)\n",
    "j_range = np.arange(0,15)\n",
    "tau_opt, j_opt, tau_l2_opt, j_l2_opt = gen_opt_G(train, tau_range, j_range, add_hist = False, subtract = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6218705-06e0-43ad-b118-e5c5475f4e3e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def gen_opt_G(train, tau_range, j_range, add_hist = False, subtract = False, verbose = False):\n",
    "    \n",
    "    print(f'Optimizing G for {train}...')\n",
    "    \n",
    "    # Load required data\n",
    "    A = ERFutils.A\n",
    "    tas_ds, tas_glob_ds, ERF_ds = load_G_data(train, add_hist = add_hist)\n",
    "    tas_ds = tas_ds.sel(s = slice(ERF_ds['year'].min(), ERF_ds['year'].max()))\n",
    "    tas_glob_ds = tas_glob_ds.sel(year = slice(ERF_ds['year'].min(), ERF_ds['year'].max()))\n",
    "    X = ERF_ds.year.values\n",
    "    \n",
    "    # Used for local GFs, not necessary to recompute\n",
    "    response_stacked = tas_ds.stack(allpoints=['lat','lon'])\n",
    "    N_latlong = len(response_stacked.tas.values[0])\n",
    "    tas_stacked = response_stacked.tas.values\n",
    "    \n",
    "    # Generate random indices to evaluate\n",
    "    n_rand = 1000\n",
    "    rand_ind = np.random.randint(N_latlong, size=(1, n_rand))\n",
    "    \n",
    "    # Instantiate vars\n",
    "    j_opt, tau_opt, MAE_opt = -1, -1, np.inf\n",
    "    j_l2_opt, tau_l2_opt, l2_opt = -1, -1, np.inf\n",
    "    \n",
    "    # Iterate over smoothing parameters\n",
    "    for tau in tau_range:\n",
    "        MAE_loc_opt = np.inf\n",
    "        for j in j_range:\n",
    "            if add_hist:\n",
    "                j += 150\n",
    "\n",
    "            if subtract:\n",
    "                Y1 = ERF_ds.ERF.values - ERF_ds.ERF.values[j-1]\n",
    "                Y2 = tas_glob_ds.tas.values - tas_glob_ds.tas.values[j-1]\n",
    "\n",
    "            else:\n",
    "                Y1 = ERF_ds.ERF.values\n",
    "                Y2 = tas_glob_ds.tas.values\n",
    "    \n",
    "            # Smooth data and create ERF matrix\n",
    "            N_years = len(ERF_ds['year']) - j\n",
    "            offsets = [i for i in range(0,-N_years,-1)]\n",
    "            domain = np.linspace(ERF_ds.year.values[j], ERF_ds.year.values[-1], num=N_years)\n",
    "            \n",
    "            # Smooth data via local weighted regression\n",
    "            ERF_smooth = [local_weighted_regression(x0, X, Y1, tau) for x0 in domain]\n",
    "            tas_glob_smooth = [local_weighted_regression(x0, X, Y2, tau) for x0 in domain]\n",
    "            ERF_matrix = diags(ERF_smooth,offsets=offsets,shape=(N_years,N_years),format='csr')\n",
    "    \n",
    "            # System should be well conditioned for linear solve\n",
    "            cond_num = LA.cond(ERF_matrix.toarray())\n",
    "            if cond_num > 10000: continue\n",
    "\n",
    "            # Calculate global GF to check if this combination is worth evaluating spatially\n",
    "            G_glob = spsolve_triangular(ERF_matrix,tas_glob_smooth,lower=True)\n",
    "\n",
    "            # Function should be monotonically decreasing and non-negative\n",
    "            if G_glob[0] < G_glob[1]: continue\n",
    "            \n",
    "            if any(G_glob[0:20] < 0): continue\n",
    "\n",
    "            # If all constraints are satisfied, continue to local GFs\n",
    "            G_stacked = np.zeros((N_years,N_latlong))\n",
    "\n",
    "            ## Evaluate random subset of GFs\n",
    "            for n in rand_ind:\n",
    "                if subtract:\n",
    "                    Y3 = tas_stacked[:,n] - tas_stacked[j-1,n]\n",
    "                else:\n",
    "                    Y3 = tas_stacked[:,n]\n",
    "                tas_loc_smooth = [local_weighted_regression(x0, X, Y3, tau) for x0 in domain]\n",
    "                G_stacked[:,n] = spsolve_triangular(ERF_matrix,tas_loc_smooth,lower=True)\n",
    "\n",
    "            ## Create local GF datset\n",
    "            G_loc = xr.Dataset(coords={'lon': ('lon', tas_ds.lon.values),\n",
    "                                    'lat': ('lat', tas_ds.lat.values),\n",
    "                                    's': ('s', range(N_years))})\n",
    "            G_loc = G_loc.stack(allpoints=['lat','lon'])\n",
    "            G_loc['G[tas]'] = (('s','allpoints'),G_stacked)\n",
    "            G_loc = G_loc.unstack('allpoints')\n",
    "            G_loc['s'] = G_loc['s'] - G_loc['s'][0]\n",
    "            \n",
    "            conv_loc = signal.convolve(np.array(G_loc.dropna(dim = 's')['G[tas]']), \n",
    "                                       np.array(ERF_ds['ERF'])[~np.isnan(np.array(ERF_ds['ERF']))][..., None, None],\n",
    "                                       'full')\n",
    "            conv_loc = ERFutils.np_to_xr(conv_loc, G_loc, ERF_ds)\n",
    "            \n",
    "            if add_hist:\n",
    "                l2_temp = np.mean(vector_norm(tas_ds.tas.values[165:] - conv_loc[165:len(X)],dim=['lat','lon']).weighted(A).mean(dim = ['lat','lon']))\n",
    "                MAE_temp = np.mean(np.abs(tas_ds.tas.values[165:] - conv_loc[165:len(X)]).weighted(A).mean(dim = ['lat','lon']))\n",
    "            else:\n",
    "                l2_temp = math.sqrt(np.square(tas_ds.tas.values - conv_loc[0:len(X)]).weighted(A).mean(dim = ['lat','lon']).mean(dim = ['s']))\n",
    "                MAE_temp = np.mean(np.abs(tas_ds.tas.values - conv_loc[0:len(X)]).weighted(A).mean(dim = ['lat','lon']))\n",
    "\n",
    "            if MAE_temp < MAE_opt:\n",
    "                j_opt, tau_opt, MAE_opt = j, tau, MAE_temp\n",
    "                print(f'New global optimum found, MAE = {round(float(MAE_opt),4)}, tau = {tau}, j = {j}')\n",
    "                \n",
    "            if l2_temp < l2_opt:\n",
    "                j_l2_opt, tau_l2_opt, l2_opt = j, tau, l2_temp\n",
    "                print(f'New global optimum found, L2 = {round(float(l2_opt),4)}, tau = {tau}, j = {j}')\n",
    "            \n",
    "            if verbose:\n",
    "                if MAE_temp < MAE_loc_opt:\n",
    "                    MAE_loc_opt = MAE_temp\n",
    "                    print(f'\\tNew local optimum found, MAE = {round(float(MAE_opt),4)}, tau = {tau}, j = {j}')\n",
    "    \n",
    "    return tau_opt, j_opt, tau_l2_opt, j_l2_opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19c5ee5f-d44f-4119-958f-0ac1e7d7b2cc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_id = ['ssp370']#,'ssp245','ssp370','ssp585','1pctCO2']\n",
    "from numpy import linalg as LA\n",
    "from scipy.linalg import solve\n",
    "from scipy.sparse.linalg import spsolve\n",
    "import random\n",
    "\n",
    "A = ERFutils.A\n",
    "output_path = ERFutils.path_to_ERF_outputs\n",
    "\n",
    "for train in train_id:\n",
    "    # Load ERF data\n",
    "    ERF = {}\n",
    "    ERF_path = f'{output_path}ERF/ERF_{train}_all_ds.nc4'\n",
    "    \n",
    "    ERF_path_hist = f'{output_path}ERF/ERF_historical_all_ds.nc4'\n",
    "    ERF_ssp = xr.open_dataset(ERF_path)\n",
    "    ERF_hist = xr.open_dataset(ERF_path_hist)\n",
    "    ERF_ds = xr.concat([ERF_hist,ERF_ssp.assign_coords(year = range(165,250))],dim = 'year')\n",
    "    \n",
    "    #ERF_ds = xr.open_dataset(ERF_path)\n",
    "    \n",
    "    ERF[train] = ERFutils.ds_to_dict(ERF_ds)\n",
    "    \n",
    "    tas_path = f'{output_path}tas/tas_CMIP_{train}_all_ds.nc4'\n",
    "    tas_ds = xr.open_dataset(tas_path)\n",
    "    \n",
    "    ERF_all = ERFutils.concat_multirun(ERF[train],'model').mean(dim = 'model')\n",
    "    tas_all = tas_ds.mean(dim = ['model'])\n",
    "    #if 'ssp' in train:\n",
    "    #    tas_all = tas_all.sel(s = slice(165,250)).assign_coords(s = range(0,85))\n",
    "\n",
    "    tas_all = tas_all.rename({'s': 'year'})    \n",
    "    tas_all = tas_all.sel(year = slice(ERF_all['year'].min(), ERF_all['year'].max()))\n",
    "    \n",
    "    tau = 16\n",
    "    j = 160\n",
    "    m = j - 1 \n",
    "\n",
    "    X1 = ERF_all.year.values\n",
    "    Y1 = ERF_all.ERF.values - ERF_all.ERF.values[m]\n",
    "\n",
    "    X2 = tas_all.year.values\n",
    "    Y2 = tas_all.weighted(A).mean(dim = ['lat','lon']).tas.values - tas_all.weighted(A).mean(dim = ['lat','lon']).tas.values[m]\n",
    "\n",
    "    N_years = len(ERF_all['year']) - j\n",
    "    offsets = [i for i in range(0,-N_years,-1)]\n",
    "    domain = np.linspace(ERF_all.year.values[j], ERF_all.year.values[-1], num=N_years)\n",
    "\n",
    "    ERF_pred = [local_weighted_regression(x0, X1, Y1, tau) for x0 in domain]\n",
    "    tas_pred = [local_weighted_regression(x0, X2, Y2, tau) for x0 in domain]\n",
    "\n",
    "    input_matrix = diags(ERF_pred,offsets=offsets,shape=(N_years,N_years),format='csr')\n",
    "    G_glob = spsolve_triangular(input_matrix,tas_pred,lower=True)\n",
    "\n",
    "    array_mat = input_matrix.toarray()\n",
    "    cond_num = LA.cond(array_mat)\n",
    "    if cond_num > 10000:\n",
    "        continue\n",
    "\n",
    "    print(f'j: {j}, \\t Cond: {cond_num}')\n",
    "\n",
    "    # Have to create the Green's functions locally, stack data array\n",
    "    stacked_response = tas_all.stack(allpoints=['lat','lon'])\n",
    "    N_latlong = len(stacked_response.tas.values[0])\n",
    "    stacked_tas = stacked_response.tas.values\n",
    "\n",
    "    # Convert to np arrays, xarray indexing is too slow\n",
    "    G_stacked = np.zeros((N_years,N_latlong))\n",
    "\n",
    "    # Calculate local Green's functions, matrix is LD by construction\n",
    "    for k in range(1000): \n",
    "        n = random.randint(0,N_latlong)\n",
    "        Y3 = stacked_tas[:,n] - stacked_tas[m,n]\n",
    "        stacked_response_local = [local_weighted_regression(x0, X2, Y3, tau) for x0 in domain]\n",
    "        G_stacked[:,n] = spsolve_triangular(input_matrix,stacked_response_local,lower=True)\n",
    "\n",
    "    # Get G into the correct format\n",
    "    G = xr.Dataset(coords={'lon': ('lon', tas_all.lon.values),\n",
    "                            'lat': ('lat', tas_all.lat.values),\n",
    "                            'year': ('year', range(N_years))})\n",
    "    G = G.stack(allpoints=['lat','lon'])\n",
    "    G['G[tas]'] = (('year','allpoints'),G_stacked)\n",
    "    G = G.unstack('allpoints')\n",
    "\n",
    "    G['year'] = G['year'] - G['year'][0]\n",
    "\n",
    "    G_glob2 = G.weighted(A).mean(dim = ['lat','lon'])['G[tas]']  \n",
    "    plt.plot(G_glob,label=f'{tau}, {j}')\n",
    "    plt.plot(G_glob2/G_glob2[0]*G_glob[0])\n",
    "\n",
    "plt.legend()\n",
    "\n",
    "# 1pctCO2 no zero (manual) - tau = 20, j = 4 (0.464) *currently in paper, best performing*\n",
    "# 1pctCO2 no zero - tau = 9, j = 2 (0.4105)\n",
    "# 1pctCO2 zero - tau = 11, j = 4 (0.817)\n",
    "\n",
    "# ssp370 zero, hist - tau = 20, j = 156 (0.409)\n",
    "# ssp370 zero, no hist - tau = 8, j = 9 (1.65)\n",
    "# ssp370 no zero, hist - tau = 5, j = 148 (0.854) \n",
    "# ssp370 no zero, no hist - tau = 5, j = 0 (0.851) *currently in paper (basically)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2150a2f5-ecba-4fc3-a0db-d2c897f348ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Have to create the Green's functions locally, stack data array\n",
    "stacked_response = tas_all.stack(allpoints=['lat','lon'])\n",
    "N_latlong = len(stacked_response.tas.values[0])\n",
    "stacked_tas = stacked_response.tas.values\n",
    "\n",
    "# Convert to np arrays, xarray indexing is too slow\n",
    "G_stacked = np.zeros((N_years,N_latlong))\n",
    "\n",
    "# Calculate local Green's functions, matrix is LD by construction\n",
    "for n in range(N_latlong):\n",
    "    if n % 5000 == 0:\n",
    "        print(n)\n",
    "    Y3 = stacked_tas[:,n] - stacked_tas[m,n]\n",
    "    stacked_response_local = [local_weighted_regression(x0, X2, Y3, tau) for x0 in domain]\n",
    "    G_stacked[:,n] = spsolve_triangular(input_matrix,stacked_response_local,lower=True)\n",
    "\n",
    "# Get G into the correct format\n",
    "G = xr.Dataset(coords={'lon': ('lon', tas_all.lon.values),\n",
    "                        'lat': ('lat', tas_all.lat.values),\n",
    "                        'year': ('year', range(N_years))})\n",
    "G = G.stack(allpoints=['lat','lon'])\n",
    "G['G[tas]'] = (('year','allpoints'),G_stacked)\n",
    "G = G.unstack('allpoints')\n",
    "\n",
    "G['year'] = G['year'] - G['year'][0]\n",
    "\n",
    "G.to_netcdf(f'{output_path}GFs/G_loess_h_z_{train}_ERF_mean_ds.nc4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24f50fff-5184-4b3a-8f77-11cfb1e90a85",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10,5))\n",
    "plt.plot(G_glob)\n",
    "plt.plot(G.weighted(A).mean(dim = ['lat','lon'])['G[tas]'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gchp",
   "language": "python",
   "name": "gchp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
